{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80ac5505",
   "metadata": {},
   "source": [
    "<img style=\"float: right; margin: 30px 15px 15px 15px;\" src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTFzQj91sOlkeDFkg5HDbjtR4QJYmLXkfMNig&usqp=CAU\" width=\"800\" height=\"200\" /> \n",
    "    \n",
    "    \n",
    "### <font color='navy'> Modelos no Lineales para Pronósticos. </font>\n",
    "\n",
    "**Nombres:**\n",
    "> `Cárdenas Gallardo Paula Daniela` | `733720` <br> `Haces López José Manuel` | `734759` <br> `Villa Domínguez Paulo Adrián` | `733773`\n",
    "\n",
    "**Fecha:** Jueves 11 de Mayo de 2023\n",
    "    \n",
    "**Profesor:** Óscar David Jaramillo Zuluaga.\n",
    "    \n",
    "**Link Github**: [github.com](https://github.com/paucardenasg/Proyecto_MNLP)\n",
    "\n",
    "# <font color='maroon'> Proyecto Final </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from pmdarima.arima import auto_arima\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler, StandardScaler\n",
    "\n",
    "# Funciones y Clases\n",
    "from Utils_Multi import *\n",
    "from Utils_Lineal import *\n",
    "from Utils_Clasificacion import *\n",
    "from Utils_Univariado import NN_maker\n",
    "\n",
    "# Filtrando las advertencias\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "data = pd.read_csv('./Data/dataset.csv')\n",
    "# Eliminando la columna de index y Unit\n",
    "data.drop(columns=['SN', 'Unit'], inplace=True)\n",
    "# Poniendo el date a formato de fecha\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "# Poniendo la fecha como str\n",
    "data['Fecha_Str'] = data['Date'].dt.strftime('%Y-%m-%d')\n",
    "# Cambiando el nombre de las columnas a español\n",
    "data.rename(columns={'Date':'Fecha', 'Commodity':'Producto', 'Minimum':'Mínimo', 'Maximum':'Máximo', 'Average':'Promedio'}, inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Cargando el dataset del cambio de Rupia Nepali a Pesos\n",
    "data_cambio = pd.read_excel('./Data/CambioMoneda.xlsx')\n",
    "# Poniendo Fecha en formato de fech\n",
    "data_cambio['Fecha'] = pd.to_datetime(data_cambio['Fecha'])\n",
    "# Quitando la hora de la fecha\n",
    "data_cambio['Fecha'] = data_cambio['Fecha'].dt.date\n",
    "# Poniendo la fecha como str\n",
    "data_cambio['Fecha_Str'] = data_cambio['Fecha'].astype(str)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Obtener las clases de productos\n",
    "productos_clase = data[['Producto']]\n",
    "productos_clase = productos_clase.drop_duplicates()\n",
    "# Tomar solo la primera palabra del valor de la columna de producto\n",
    "productos_clase['Clase'] = productos_clase['Producto'].str.split(' ').str[0]\n",
    "productos_clase['Clase'] = productos_clase['Clase'].str.split('(').str[0]\n",
    "\n",
    "# Mergeando los datos con la clase de producto\n",
    "data = data.merge(productos_clase, on='Producto')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Juntando los dos datasets de tipo de cambio a pesos\n",
    "data = data.merge(data_cambio.drop(columns=['Fecha']), on='Fecha_Str')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Dataset con el precio del dolar\n",
    "dolar = pd.read_excel('./Data/PrecioDolar.xlsx')\n",
    "dolar['Fecha'] = pd.to_datetime(dolar['Fecha'])\n",
    "# Quitando la hora de la fecha\n",
    "dolar['Fecha'] = dolar['Fecha'].dt.date\n",
    "# Poniendo la fecha como str\n",
    "dolar['Fecha_Str'] = dolar['Fecha'].astype(str)\n",
    "\n",
    "# Mergeando los datos con el precio del dolar\n",
    "data = data.merge(dolar.drop(columns=['Fecha']), on='Fecha_Str')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Multiplicando el precio por el cambio de moneda\n",
    "data['Mínimo'] = np.round(data['Mínimo'] * data['Valor'], decimals=4)\n",
    "data['Máximo'] = np.round(data['Máximo'] * data['Valor'], decimals=4)\n",
    "data['Promedio'] = np.round(data['Promedio'] * data['Valor'], decimals=4)\n",
    "# Eliminar la columna de valor\n",
    "data.drop(columns=['Valor'], inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Sacando una columna con el año\n",
    "data['Year'] = data['Fecha'].dt.year\n",
    "\n",
    "# Dataset con la inflación\n",
    "inflacion = pd.read_excel('./Data/Inflacion.xlsx')\n",
    "data = data.merge(inflacion, on='Year')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Agregando el porcentaje de desempleo\n",
    "desempleo = pd.read_excel('./Data/Desempleo.xlsx')\n",
    "data = data.merge(desempleo, on='Year')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Poniendo la fecha como indice\n",
    "data.set_index('Fecha', inplace=True)\n",
    "\n",
    "# Ordenando los datos por fecha y por producto\n",
    "data.sort_values(by=['Fecha', 'Producto'], inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Ordenando las columnas\n",
    "data = data[['Producto', 'Clase', 'Mínimo', 'Máximo', 'Promedio', 'Inflacion', 'Precio_Dolar', 'Desempleo']]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f318291",
   "metadata": {},
   "source": [
    "## <font color='maroon'> EDA </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conocer las variables\n",
    "\n",
    "# Tipo de cada variable\n",
    "print(f'\\n+ Tipo de datos por columna: \\n{data.dtypes}')\n",
    "\n",
    "# Conteo de valores nulos\n",
    "print(f'\\n+ Cantidad de nulos por columna: \\n{data.isnull().sum()}')\n",
    "\n",
    "# Valores únicos\n",
    "print(f'\\n+ Valores únicos por columna: \\n{data.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e8bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos estadísticos de las variables\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064afe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrando fechas de inicio y fin de los datos, puras fechas sin hora\n",
    "print('Fecha de inicio: ', data.index.min().date())\n",
    "print('Fecha de fin: ', data.index.max().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379578e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productos\n",
    "tipos_de_productos = data['Producto'].unique()\n",
    "print(f'Cantidad de Productos: {len(tipos_de_productos)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porcentaje que representa cada producto\n",
    "porcentaje = data['Producto'].value_counts(normalize=True).reset_index().sort_values(by='Producto', ascending=False)\n",
    "# Multiplicando por 100 para obtener el porcentaje\n",
    "porcentaje['Producto'] = np.round(porcentaje['Producto'] * 100, decimals=3)\n",
    "# Haciendo una suma acumulativa\n",
    "porcentaje['Cum_Sum'] = porcentaje['Producto'].cumsum()\n",
    "porcentaje"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c91d8c3",
   "metadata": {},
   "source": [
    "___\n",
    "## <font color='maroon'> Modelos Lineales </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0316d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poniendo un seed para que siempre se obtenga el mismo resultado\n",
    "np.random.seed(21)\n",
    "\n",
    "# Random choice para seleccionar un producto al azar\n",
    "producto = np.random.choice(tipos_de_productos)\n",
    "\n",
    "# Graficando el promedio del producto seleccionado\n",
    "data[data['Producto'] == producto]['Promedio'].plot(figsize=(15, 5), title=f'Precio promedio del producto {producto} en Nepal.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629365da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la serie de tiempo de interés\n",
    "data_prod = pd.DataFrame(data.loc[data.Producto == producto]['Promedio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver si los meses están completos\n",
    "print(f'Primera fecha: {data_prod.index[0]}\\nÚltima fecha: {data_prod.index[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lin = data_prod.drop(['2013-06-16', '2013-06-17', '2013-06-18', '2013-06-19', '2013-06-20', '2013-06-21',\n",
    "                            '2013-06-25', '2013-06-26', '2013-06-27', '2013-06-28', '2013-06-30', '2021-05-01',\n",
    "                            '2021-05-02', '2021-05-03', '2021-05-04', '2021-05-05', '2021-05-06', '2021-05-07',\n",
    "                            '2021-05-08', '2021-05-09', '2021-05-10', '2021-05-11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eae5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener un promedio mensual de los precios en lugar de mantener datos diarios\n",
    "data_lin = data_lin.resample('M').mean()\n",
    "data_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff457093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en datos de entrenamiento (80%) y prueba (20%)\n",
    "div = int(data_lin.shape[0]*.8)\n",
    "train = data_lin[:div+1]\n",
    "test = data_lin[div:]\n",
    "\n",
    "# Visualizar partición entrenamiento - prueba\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "train.plot(figsize=(8,5), ax=ax,)\n",
    "test.plot(figsize=(8,5), ax=ax)\n",
    "ax.legend(labels = ['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la distribución de la serie de tiempo\n",
    "fig, ax = plt.subplots(3, 1, figsize=(8, 8))\n",
    "sns.boxplot(x=train['Promedio'], ax=ax[0])\n",
    "sns.histplot(x=train['Promedio'], ax=ax[1])\n",
    "sns.lineplot(data=train['Promedio'], ax=ax[2])\n",
    "ax[0].set(title='Boxplot', xlabel=None)\n",
    "ax[1].set(title='Histograma', xlabel=None)\n",
    "ax[2].set(title='Diagrama de línea', xlabel=None)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de9649f9",
   "metadata": {},
   "source": [
    "Dado que la serie no tiene una distribución normal, se transformará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd318aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_series(train_data, method, plot=True):\n",
    "    data_transformations = train_data.copy()\n",
    "    # Escalamientos\n",
    "    if method == 'min_max':\n",
    "        min_max = MinMaxScaler()\n",
    "        data_transformations['min_max'] = min_max.fit_transform(data_transformations.values.reshape(-1, 1))\n",
    "    elif method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "        data_transformations['standar'] = scaler.fit_transform(data_transformations.values.reshape(-1, 1))\n",
    "    elif method == 'log':\n",
    "        data_transformations['log'] = np.log(data_transformations)\n",
    "    elif method == 'box_cox':\n",
    "        data_transformations['box_cox'] = power_transform(data_transformations.values.reshape(-1, 1), method='box-cox')\n",
    "    else:\n",
    "        raise ValueError('Método de transformación no válido.')\n",
    "    print(f'Transformación {method} completada.')\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(3, 1, figsize=(8, 8))\n",
    "        sns.boxplot(x=data_transformations[method], ax=ax[0])\n",
    "        sns.histplot(x=data_transformations[method], ax=ax[1])\n",
    "        sns.lineplot(data=data_transformations[method], ax=ax[2])\n",
    "        ax[0].set(title=\"Boxplot\", xlabel=None)\n",
    "        ax[1].set(title=\"Histograma\", xlabel=None)\n",
    "        ax[2].set(title=\"Diagrama de línea\", xlabel=None)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "    return data_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = transform_series(train, 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eae702",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = seasonal_decompose(train['log'], model='additive', period=12)\n",
    "\n",
    "# Gráfica\n",
    "fig, ax = plt.subplots(4, sharex=True, figsize=(8, 8))\n",
    "resultados.observed.plot(ax=ax[0])   # Datos originales\n",
    "ax[0].set_ylabel('Observed')\n",
    "resultados.trend.plot(ax=ax[1])      # Tendencia\n",
    "ax[1].set_ylabel('Trend')\n",
    "resultados.seasonal.plot(ax=ax[2])   # Estacionalidad\n",
    "ax[2].set_ylabel('Seasonal')\n",
    "resultados.resid.plot(ax=ax[3])      # Residuos\n",
    "ax[3].set_ylabel('Residual')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f81dc6a6",
   "metadata": {},
   "source": [
    "La serie de tiempo muestra una clara estacionalidad anual (cada 12 meses), así como una ligera tendencia a la alcista que era de esperarse por la inflación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar autocorrelación\n",
    "def plot_acf_pacf(data, kwargs=dict()):\n",
    "    f = plt.figure(figsize=(8,5))\n",
    "    ax1 = f.add_subplot(121)\n",
    "    plot_acf(data, zero=False, ax=ax1, **kwargs)\n",
    "    ax2 = f.add_subplot(122)\n",
    "    plot_pacf(data, zero=False, ax=ax2, method='ols', **kwargs)\n",
    "    plt.show()\n",
    "    \n",
    "# Función para realizar la prueba de Dikcey-Fuller\n",
    "def adf_test(timeseries):\n",
    "    print(\"Results of Dickey-Fuller Test:\")\n",
    "    dftest = adfuller(timeseries, autolag=\"AIC\")\n",
    "    dfoutput = pd.Series(\n",
    "        dftest[0:4],\n",
    "        index=[\n",
    "            \"Test Statistic\",\n",
    "            \"p-value\",\n",
    "            \"#Lags Used\",\n",
    "            \"Number of Observations Used\",\n",
    "        ],\n",
    "    )\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput[\"Critical Value (%s)\" % key] = value\n",
    "    print(dfoutput)\n",
    "    \n",
    "    if (dftest[1] <= 0.05) & (dftest[4]['5%'] > dftest[0]):\n",
    "        print(\"\\u001b[32mStationary\\u001b[0m\")\n",
    "    else:\n",
    "        print(\"\\x1b[31mNon-stationary\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de autocorrelación\n",
    "plot_acf_pacf(train['log'], {'lags':25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de Dikcey-Fuller para ver si la serie es estacionaria\n",
    "adf_test(train['log'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5ddfa78",
   "metadata": {},
   "source": [
    "Como la serie no es estacionaria, se probarán diferenciaciones de primer y segundo orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2470cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de autocorrelación con diferenciación\n",
    "diff1 = train['log'].diff().dropna()\n",
    "print('ADF para derivada primer orden\\n')\n",
    "adf_test(diff1)\n",
    "\n",
    "diff2 = train['log'].diff().diff().dropna()\n",
    "print('ADF para derivada segundo orden\\n')\n",
    "adf_test(diff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b0e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de autocorrelación con diferenciación\n",
    "plot_acf_pacf(diff1, {'lags':25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de autocorrelación con diferenciación de segundo orden\n",
    "plot_acf_pacf(diff2, {'lags':25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte estacional\n",
    "plot_acf_pacf(resultados.seasonal, {'lags':25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte estacional con diferenciación\n",
    "plot_acf_pacf(resultados.seasonal.diff().dropna(), {'lags':25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a18d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte estacional con diferenciación\n",
    "plot_acf_pacf(resultados.seasonal.diff().diff().dropna(), {'lags':25})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a058828",
   "metadata": {},
   "source": [
    "**Propuestas de modelos:**\n",
    "Después de analizar los datos, se considera que podrían funcionar los siguientes modelos:\n",
    "+ $ARIMA(1, 0, 3)$ porque el primer *lag* es significativo en la PACF y los primeros tres *lags* son significativos en la ACF\n",
    "+ $ARIMA(2, 2, 2)$ porque los dos primeros *lags* son sifnigicantes en ambas gráficas (PACF y ACF), además de la diferenciación de segundo orden\n",
    "+ $ARIMA(2, 1, 2)$ porque los dos primeros *lags* son sifnigicantes en ambas gráficas (PACF y ACF), además de la diferenciación\n",
    "\n",
    "Siguiendo esta lógica, también se probarán las siguientes combinaciones:\n",
    "+ $ARIMA(1, 0, 2)$\n",
    "+ $ARIMA(1, 0, 1)$\n",
    "+ $ARIMA(1, 2, 2)$\n",
    "+ $ARIMA(2, 2, 1)$\n",
    "+ $ARIMA(1, 2, 1)$\n",
    "+ $ARIMA(1, 1, 2)$\n",
    "+ $ARIMA(2, 1, 1)$\n",
    "+ $ARIMA(1, 1, 1)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15febed5",
   "metadata": {},
   "source": [
    "Además se verá qué modelo sugiere el método `autoarima`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708cd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo\n",
    "auto_model = auto_arima(train['log'],\n",
    "                        start_p=1,\n",
    "                        start_q=1,\n",
    "                        test='adf',                   # para encontrar el 'd' óptimo\n",
    "                        information_criterion='aic',  # se buscará reducir el AIC\n",
    "                        m=1,             \n",
    "                        d=1,          \n",
    "                        seasonal=True,   \n",
    "                        start_P=0, \n",
    "                        D=None, \n",
    "                        trace=True,\n",
    "                        error_action='ignore',  \n",
    "                        suppress_warnings=True, \n",
    "                        stepwise=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e28c2d64",
   "metadata": {},
   "source": [
    "**Modelado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f17b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 1\n",
    "arima_model1 = ARIMA(train['log'], order=(1, 0, 3))\n",
    "model1 = arima_model1.fit()\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 2\n",
    "arima_model2 = ARIMA(train['log'], order=(2, 2, 2))\n",
    "model2 = arima_model2.fit()\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ab263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 3\n",
    "arima_model3 = ARIMA(train['log'], order=(2, 1, 2))\n",
    "model3 = arima_model3.fit()\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a05657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 4\n",
    "arima_model4 = ARIMA(train['log'], order=(1, 0, 2))\n",
    "model4 = arima_model4.fit()\n",
    "print(model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a657a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 5\n",
    "arima_model5 = ARIMA(train['log'], order=(1, 0, 1))\n",
    "model5 = arima_model5.fit()\n",
    "print(model5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 6\n",
    "arima_model6 = ARIMA(train['log'], order=(1, 2, 2))\n",
    "model6 = arima_model6.fit()\n",
    "print(model6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 7\n",
    "arima_model7 = ARIMA(train['log'], order=(2, 2, 1))\n",
    "model7 = arima_model7.fit()\n",
    "print(model7.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 8\n",
    "arima_model8 = ARIMA(train['log'], order=(1, 2, 1))\n",
    "model8 = arima_model8.fit()\n",
    "print(model8.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 9\n",
    "arima_model9 = ARIMA(train['log'], order=(1, 1, 2))\n",
    "model9 = arima_model9.fit()\n",
    "print(model9.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 10\n",
    "arima_model10 = ARIMA(train['log'], order=(2, 1, 1))\n",
    "model10 = arima_model10.fit()\n",
    "print(model10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142846ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 11\n",
    "arima_model11 = ARIMA(train['log'], order=(1, 1, 1))\n",
    "model11 = arima_model11.fit()\n",
    "print(model11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec203ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 12\n",
    "model12 = SARIMAX(train['log'], order=(1, 1, 1), seasonal_order=(2, 0, 1, 12))\n",
    "sarima1 = model12.fit()\n",
    "sarima1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cbdefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 13\n",
    "model13 = SARIMAX(train['log'], order=(0, 1, 0), seasonal_order=(1, 0, 1, 12))\n",
    "sarima2 = model13.fit()\n",
    "sarima2.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b531d4ce",
   "metadata": {},
   "source": [
    "**Ahora, se analizarán los resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para separar los valores del resumen del modelo\n",
    "def extract_values(text):\n",
    "    values = {}\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            words = line.split()\n",
    "            if len(words) >= 2:\n",
    "                try:\n",
    "                    value = float(words[-1])\n",
    "                    key = \" \".join(words[:-1])\n",
    "                    values[key] = value\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aedd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista con todos los resultados de los modelos\n",
    "results = []\n",
    "for i in [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, sarima1, sarima2]:\n",
    "    results.append(extract_values(i.summary().as_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a DataFrame\n",
    "r = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Visualizar\n",
    "br1 = np.arange(len(list(r.iloc[:, 0])))\n",
    "br2 = [x + 0.25 for x in br1]\n",
    "plt.bar(br1, r.iloc[:, 2], color ='teal', width = 0.25, label ='AIC')\n",
    "plt.bar(br2, r.iloc[:, 3], color ='cornflowerblue', width = 0.25, label ='BIC')\n",
    "plt.title('Complejidad y desempeño de modelos')\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Valor')\n",
    "plt.xticks([r + 0.25 for r in range(13)], ['arima_1', 'arima_2', 'arima_3', 'arima_4', 'arima_5', 'arima_6', 'arima_7',\n",
    "                                           'arima_8', 'arima_9', 'arima_10', 'arima_11', 'sarima_1', 'sarima_2'],\n",
    "           rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d6a73a3",
   "metadata": {},
   "source": [
    "**_`Observaciones:`_** <br>\n",
    "+ **Log-Likelihood**: conforme el valor de la verosimilitud sea mayor, es mejor. En este caso, todos los modelos estuvieron entre $-3$ y $-12$. El mejor fue el model 12 (primer SARIMAX). \n",
    "\n",
    "+ **Criterio de información de Akaike**: si este valor es menor, es mejor. Por lo podemos *rankear* a los modelos de la siguiente manera:\n",
    "1. Modelo 9 ($18.103$)\n",
    "1. Modelo 5 ($18.128$)\n",
    "1. Modelo 12 ($18.482$)\n",
    "1. Modelo 4 ($18.977$)\n",
    "1. Modelo 13 ($18.987$)\n",
    "1. Modelo 3 ($19.784$)\n",
    "1. Modelo 1 ($20.950$)\n",
    "1. Modelo 10 ($22.587$)\n",
    "1. Modelo 11 ($23.825$)\n",
    "1. Modelo 7 ($27.898$)\n",
    "1. Modelo 2 ($29.803$)\n",
    "1. Modelo 6 ($30.594$)\n",
    "\n",
    "+ **Criterio de información bayesiano**: si este valor es menor, es mejor. Por lo podemos *rankear* a los modelos de la siguiente manera:\n",
    "1. Modelo 12 ($24.034$)\n",
    "1. Modelo 13 ($25.940$)\n",
    "1. Modelo 11 ($30.778$)\n",
    "1. Modelo 9 ($27.372$)\n",
    "1. Modelo 5 ($27.451$)\n",
    "1. Modelo 4 ($30.631$)\n",
    "1. Modelo 3 ($31.372$)\n",
    "1. Modelo 10 ($31.857$)\n",
    "1. Modelo 1 ($34.935$)\n",
    "1. Modelo 7 ($37.114$)\n",
    "1. Modelo 8 ($38.262$)\n",
    "1. Modelo 6 ($39.811$)\n",
    "1. Modelo 2 ($41.324$)\n",
    "\n",
    "+ **Ljung-Box**: en todos los casos el *p-value* es mayor a $0.05$, así que no se rechaza la hipótesis nula y los datos se distribuyen de forma independiente\n",
    "+ **Heterocedasticidad**: en la mayoría de los casos el *p-value* es mayor a $0.05$ y no se rechaza la hipótesis nula, por lo que los residuos muestran varianza cambiante a excepción del modelo 3 y 7\n",
    "+ **Jarque-Bera**: todos los modelos tienen un *p-value* es mayor a $0.05$ en esta prueba, por lo que no se rechaza la hipótesis nula y los datos se distribuyen normalmente\n",
    "\n",
    "El mejor modelo parece ser el 12 ($SARIMA(1, 1, 1) \\times (2, 0, 1, 12)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c676d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima1.plot_diagnostics(figsize=(10, 10))\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38b98e07",
   "metadata": {},
   "source": [
    "**`Observaciones:`**\n",
    "+ Se pueden ver los residuales sin tendencia ni estacionalidad\n",
    "+ En el histograma se puede ver que los residuales no son muy distintos a una distribución normal, lo cual es muy bueno\n",
    "+ En la gráfica `Normal Q-Q` podemos ver que los residuales tienen un comportamiento similar a la línea de referencia\n",
    "+ Los residuales no tienen correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20479377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción\n",
    "# Pronóstico\n",
    "y_h = np.exp(sarima1.predict(start=train.shape[0], end=train.shape[0]+test.shape[0], dynamic=False)).to_frame()\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "train['Promedio'].plot(ax = ax)\n",
    "test.plot(ax = ax)\n",
    "y_h.plot(ax = ax)\n",
    "ax.legend(labels = ['train', 'test', 'forecast'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94647143",
   "metadata": {},
   "source": [
    "___\n",
    "## <font color='maroon'> Parte Univariada con Deep Learning </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4609b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para la parte univariada solo con la variable del promedio.\n",
    "df = data_prod[[\"Promedio\"]]\n",
    "obj = NN_maker(data=df, n_steps= 7, horizont=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.plot_serie()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bde4030",
   "metadata": {},
   "source": [
    "Al estar tan sesgada la serie, vamos a aplicar un logaritmo, a ver si mejora, ya que tiene una cola positiva, el logaritmo puede tender a mejorar la distribución de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.transform_data()\n",
    "obj.plot_serie()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "001814a4",
   "metadata": {},
   "source": [
    "En efecto ahora hay una distribución mucho más normal, vamos a proceder con el modelado de los datos, primero vamos a hacer un modelo muy simple mlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfeeda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos los datos en train, val y test\n",
    "X_train, X_val, X_test, y_train, y_val, y_test= obj.train_val_test_split()\n",
    "\n",
    "# Imprimir dimensiones de los datos de entrenamiento, validación y test\n",
    "print('Datos de entrenamiento', X_train.shape, y_train.shape)\n",
    "print('Datos de validación', X_val.shape, y_val.shape)\n",
    "print('Datos de test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos un modelo sencillo con solo 1 capa oculta y 32 neuronas\n",
    "num_hidden_layers = 1\n",
    "num_neurons = 32\n",
    "dropout_rate = None\n",
    "\n",
    "model_mlp_1 = obj.MLP_builder(num_hidden_layers=num_hidden_layers, X_train=X_train, X_val=X_val, X_test=X_test, y_train=y_train, \n",
    "                        y_val=y_val, y_test=y_test, num_neurons=num_neurons, log=True, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ec72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora corremos con 2 capas ocultas y 64 neuronas, agregamos dropout para evitar overfitting\n",
    "num_hidden_layers = 2\n",
    "num_neurons = 64\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model_mlp_2 = obj.MLP_builder(num_hidden_layers=num_hidden_layers, X_train=X_train, X_val=X_val, X_test=X_test, y_train=y_train, \n",
    "                        y_val=y_val, y_test=y_test, num_neurons=num_neurons, log=True, plot=True, dropout_rate=dropout_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a161175",
   "metadata": {},
   "source": [
    "Parece que las capas ocultas evitaron que siga aprendiendo, voy a intentar, dejando el dropout y quitando una oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a quitar una capa oculta y jugar con el dropout.\n",
    "num_hidden_layers = 1\n",
    "num_neurons = 64\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model_mlp_3 = obj.MLP_builder(num_hidden_layers=num_hidden_layers, X_train=X_train, X_val=X_val, X_test=X_test, y_train=y_train, \n",
    "                        y_val=y_val, y_test=y_test, num_neurons=num_neurons, log=True, plot=True, dropout_rate=dropout_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eb7f658",
   "metadata": {},
   "source": [
    "### <font color='teal'> CNN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b946262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos en train, val y test. y reordenamos en forma tensorial con el parametro conv.\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = obj.train_val_test_split(conv=True)\n",
    "\n",
    "# Imprimir dimensiones de los datos de entrenamiento, validación y test\n",
    "print('Datos de entrenamiento', X_train.shape, y_train.shape)\n",
    "print('Datos de validación', X_val.shape, y_val.shape)\n",
    "print('Datos de test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos nuestra primer red, solo una capa oculta\n",
    "num_hidden_layers = 1\n",
    "num_neurons = 256\n",
    "num_filters = 64\n",
    "kernel_size = 2\n",
    "\n",
    "model_cnn_1 = obj.cnn_builder(num_hidden_layers=num_hidden_layers, num_neurons=num_neurons,\n",
    "                                    num_filters=num_filters, kernel_size=kernel_size, pool_size=2,\n",
    "                                    X_train=X_train, X_val=X_val, X_test=X_test, y_train=y_train,\n",
    "                                    y_val=y_val, y_test=y_test, plot=True, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e882b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dió muy buen resultado, pero los picos todavía no me convencen\n",
    "num_hidden_layers = 1\n",
    "num_neurons = 256\n",
    "num_filters = 64\n",
    "kernel_size = 2\n",
    "\n",
    "model_cnn_2 = obj.cnn_builder(num_hidden_layers=num_hidden_layers, num_neurons=num_neurons,\n",
    "                                    num_filters=num_filters, kernel_size=kernel_size, pool_size=2,\n",
    "                                    X_train=X_train, X_val=X_val, X_test=X_test, y_train=y_train,\n",
    "                                    y_val=y_val, y_test=y_test, plot=True, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ba6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos si el dropout hace algun buen efecto en la red\n",
    "num_hidden_layers = 1\n",
    "num_neurons = 256\n",
    "num_filters = 64\n",
    "kernel_size = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model_cnn_3 = obj.cnn_builder(num_hidden_layers=num_hidden_layers, num_neurons=num_neurons,\n",
    "                                    num_filters=num_filters, kernel_size=kernel_size, pool_size=2,\n",
    "                                    X_train=X_train, X_val=X_val, X_test=X_test, y_train=y_train,\n",
    "                                    y_val=y_val, y_test=y_test, dropout_rate=dropout_rate, plot=True, log=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58af058e",
   "metadata": {},
   "source": [
    "### <font color='teal'>LSTM </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_lstm = 50\n",
    "capas_ocultas_dense = 1\n",
    "units_dense = 32\n",
    "\n",
    "model_lstm_1 = obj.lstm_builder(units_lstm=units_lstm, capas_ocultas_lstm=1, capas_ocultas_dense=capas_ocultas_dense, \n",
    "                                     units_dense=units_dense, X_train = X_train, X_val=X_val, X_test=X_test, y_train=y_train, \n",
    "                                     y_val=y_val, y_test=y_test, plot=True, dropout=None, log=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b337400f",
   "metadata": {},
   "source": [
    "Modelo muy malo, vamos a ver si con otros parámetros puede mejorar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesimo modelo, vamos agregando más parámetros\n",
    "units_lstm = 50\n",
    "capas_ocultas_dense = 3\n",
    "units_dense = 128\n",
    "\n",
    "model_lstm_2 = obj.lstm_builder(units_lstm=units_lstm, capas_ocultas_lstm=1, capas_ocultas_dense=capas_ocultas_dense, \n",
    "                                     units_dense=units_dense, X_train = X_train, X_val=X_val, X_test=X_test, y_train=y_train, y_val=y_val, \n",
    "                                     y_test=y_test, plot=True, dropout=None, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ca39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a ver si con más unidades lstm se puede mejorar el modelo\n",
    "units_lstm = 500\n",
    "capas_ocultas_dense = 5\n",
    "units_dense = 526\n",
    "\n",
    "model_lstm_3 = obj.lstm_builder(units_lstm=units_lstm, capas_ocultas_lstm=1, capas_ocultas_dense=capas_ocultas_dense, \n",
    "                                     units_dense=units_dense, X_train = X_train, X_val=X_val, X_test=X_test, y_train=y_train, y_val=y_val, \n",
    "                                     y_test=y_test, plot=True, dropout=None, log=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72a5315d",
   "metadata": {},
   "source": [
    "### <font color='teal'> CNN-LSTM </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos en train, val y test. y reordenamos en forma tensorial con el parametro conv.\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = obj.train_val_test_split(conv=True)\n",
    "\n",
    "n_features = 1\n",
    "n_seq = 1\n",
    "n_steps = 7\n",
    "X_train = X_train.reshape((X_train.shape[0], n_seq, n_steps, n_features))\n",
    "X_val = X_val.reshape((X_val.shape[0], n_seq, n_steps, n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], n_seq, n_steps, n_features))\n",
    "\n",
    "print('Datos de entrenamiento', X_train.shape, y_train.shape)\n",
    "print('Datos de validación', X_val.shape, y_val.shape)\n",
    "print('Datos de test', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012fc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empezamos \n",
    "blocks = 1\n",
    "filters = 10\n",
    "units_lstm = 30\n",
    "\n",
    "model_cnn_lstm_1 = obj.cnn_lstm_builder(blocks=blocks, filters=filters, units_lstm=units_lstm,  X_train=X_train, X_val=X_val, \n",
    "                                             X_test=X_test, y_train=y_train, y_val=y_val, y_test=y_test, plot=True, \n",
    "                                             dropout=None, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = 1\n",
    "filters = 40\n",
    "units_lstm = 50\n",
    "\n",
    "model_cnn_lstm_2 = obj.cnn_lstm_builder(blocks=blocks, filters=filters, units_lstm=units_lstm,  X_train=X_train, X_val=X_val, \n",
    "                                             X_test=X_test, y_train=y_train, y_val=y_val, y_test=y_test, plot=True, \n",
    "                                             dropout=None, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = 1\n",
    "filters = 256\n",
    "dropout_rate = 0.2\n",
    "units_lstm = 100\n",
    "\n",
    "model_cnn_lstm_3 = obj.cnn_lstm_builder(blocks=blocks, filters=filters, units_lstm=units_lstm,  X_train=X_train, X_val=X_val, \n",
    "                                             X_test=X_test, y_train=y_train, y_val=y_val, y_test=y_test, plot=True, \n",
    "                                             dropout=dropout_rate, log=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea9e5d2c",
   "metadata": {},
   "source": [
    "El mejor modelo conseguido fue una CNN, vamos a hacer un optuna con esa arquitectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Dropout, Conv1D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "import optuna\n",
    "\n",
    "# Separamos los datos en train, val y test\n",
    "X_train, X_val, X_test, y_train, y_val, y_test= obj.train_val_test_split(conv=True)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Definir los hiperparámetros que queremos optimizar\n",
    "    filters = trial.suggest_int(\"filters\", 32, 128)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "    dense_size = trial.suggest_int(\"dense_size\", 64, 256)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Construir el modelo con los hiperparámetros elegidos\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_size, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compilar el modelo con el optimizador y la función de pérdida\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    # Entrenar el modelo y obtener las predicciones\n",
    "    history = model.fit(X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=256,\n",
    "            epochs=200,\n",
    "            verbose=False)\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print('Best trial:', study.best_trial.params)\n",
    "print('Best validation loss:', study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacdd50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos los mejores parámetros\n",
    "dict_params = study.best_params\n",
    "dict_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a3d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos nuestra red con datos optimizados\n",
    "dense_size = dict_params[\"dense_size\"]\n",
    "dropout = dict_params[\"dropout\"]\n",
    "filters = dict_params[\"filters\"]\n",
    "lr = dict_params[\"learning_rate\"]\n",
    "\n",
    "# Construir el modelo con los hiperparámetros elegidos\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=filters, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_size, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compilar el modelo con el optimizador y la función de pérdida\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "obj.train_models(model=model, X_train=X_train, X_val=X_val, X_test=X_test, \n",
    "                 y_train=y_train, y_val=y_val, y_test=y_test, log=True, plot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8145fe35",
   "metadata": {},
   "source": [
    "Pues salió bastante parecido el resultado del modelo solo que con menos neuronas, filtros, etc. Entonces se puede considerar que nos funcionó el optuna."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d5c08c2",
   "metadata": {},
   "source": [
    "___\n",
    "## <font color='maroon'> Parte Multivariada con Deep Learning </font>\n",
    "### Creación del dataset\n",
    "- Al tener datos que contiene las variables de Mínimo, Inflación, Precio del Dolar y Porcentaje de Desempelo para predecir el precio promedio del producto tenemos que crear el dataset de manera que las variables predictoras sean del día anterior al que se quiere predecir.\n",
    "    - Es decir, si queremos predecir el precio promedio del producto para el día 2021-06-01, las variables predictoras serán las del día 2021-05-31.\n",
    "- Se decidió hacer una ventana de 7, al tener datos diarios hacemos que nuestras ventanas de tiempo sean semanales y podamos realizar un mejor uso de los datos.\n",
    "- Las variables:\n",
    "    - Inflación y Porcentaje de Desempleo: parecieran que son estaticas, sin embargo al tener temporalidad más grande no afectan mucho a la predicción.\n",
    "    - Mínimo y Precio del Dolar: son las variables que más afectan a la predicción, ya que son las que más varían en el tiempo y afectan directamente al precio promedio del producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd0586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos el dataset para quedarnos solo con el producto que queremos predecir\n",
    "data_prod_multi = data[data['Producto'] == producto]\n",
    "# Eliminando fechas\n",
    "data_prod_multi = data_prod_multi.drop(['2013-06-16', '2013-06-17', '2013-06-18', '2013-06-19', '2013-06-20', '2013-06-21',\n",
    "                                        '2013-06-25', '2013-06-26', '2013-06-27', '2013-06-28', '2013-06-30', '2021-05-01',\n",
    "                                        '2021-05-02', '2021-05-03', '2021-05-04', '2021-05-05', '2021-05-06', '2021-05-07',\n",
    "                                        '2021-05-08', '2021-05-09', '2021-05-10', '2021-05-11'])\n",
    "\n",
    "# Tomamos las 3 variables que se usarán como predictores y la de respuesta\n",
    "data_multi = data_prod_multi[['Mínimo', 'Inflacion', 'Precio_Dolar', 'Desempleo', 'Promedio']]\n",
    "# Desplazamos las variables predictoras 1 día\n",
    "data_multi[['Mínimo', 'Inflacion', 'Precio_Dolar', 'Desempleo']] = data_multi[['Mínimo', 'Inflacion', 'Precio_Dolar', 'Desempleo']].shift(1)\n",
    "# Quitando nulos\n",
    "data_multi = data_multi.dropna()\n",
    "# Convertimos el dataframe a un array\n",
    "data_multi = data_multi.values\n",
    "# Mostrando los tamaños\n",
    "print(f'Forma del array: \\n- {data_multi.shape}\\n')\n",
    "print(f'Primeros 5 elementos del array: \\n{data_multi[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b13c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo en X e y, además de tomar un n-steps de 7 para que sea semanal\n",
    "X, y = split_multivariate_sequence(data_multi, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split_multi(X=X, \n",
    "                                                          y=y,\n",
    "                                                          train=0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e02ca43",
   "metadata": {},
   "source": [
    "### <font color='teal'> MLP </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo de MLP con 6 capas ocultas y 64 neuronas por capa\n",
    "model_mlp_multi, history_mlp_mlti = gen_MLP_model(X=X_train, y=y_train, val_split=0.2, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                                                  activation='relu', num_layers=6, num_neurons=64,\n",
    "                                                  optimizer='Adagrad', lr=0.01, loss='mse', metrics=['mae'],\n",
    "                                                  patience=25, epochs=500, verbose=0,\n",
    "                                                  X_test=X_test, y_test=y_test, index=np.arange(0, 464),\n",
    "                                                  plot_history=True\n",
    "                                                  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c07b1f8f",
   "metadata": {},
   "source": [
    "### <font color='teal'> CNN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdacf7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo de CNN con 5 capas de convolución con 128 filtros y 5 capas densas con 100 neuronas\n",
    "model_cnn_multi, history_cnn_multi = gen_CNN_model(X=X_train, y=y_train, val_split=0.2, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                                                    num_layers_cnn=5, num_filters=128, kernel_size=4, padding='same',\n",
    "                                                    activation='relu', num_layers_dense=5, num_neurons=100,\n",
    "                                                    optimizer='Adagrad', lr=0.01, loss='mse', metrics=['mae'],\n",
    "                                                    patience=25, epochs=500, verbose=0,\n",
    "                                                    X_test=X_test, y_test=y_test, index=np.arange(0, 464),\n",
    "                                                    plot_history=True\n",
    "                                                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3b069db",
   "metadata": {},
   "source": [
    "### <font color='teal'> LSTM </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo de LSTM con 5 capas de LSTM con 100 unidades y 5 capas densas con 100 neuronas\n",
    "model_lstm_multi, history_lstm_multi = gen_LSTM_model(X=X_train, y=y_train, val_split=0.2, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                                                      num_layers_lstm=5, activation_lstm='relu', num_units_lstm=100, bidireccional=True,\n",
    "                                                      activation='relu', num_layers_dense=5, num_neurons=100,\n",
    "                                                      optimizer='Adagrad', lr=0.01, loss='mse', metrics=['mae'],\n",
    "                                                      patience=25, epochs=500, verbose=0,\n",
    "                                                      X_test=X_test, y_test=y_test, index=np.arange(0, 464),\n",
    "                                                      plot_history=True\n",
    "                                                      )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90080fce",
   "metadata": {},
   "source": [
    "### <font color='teal'> CNN-LSTM </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5fcf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm_multi, history_cnn_lstm_multi = gen_CNN_LSTM_model(X=X_train, y=y_train, val_split=0.2, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                                                                  num_layers_cnn=5, num_filters=128, kernel_size=4, padding='same',\n",
    "                                                                  num_layers_lstm=5, activation_lstm='relu', num_units_lstm=100,\n",
    "                                                                  activation='relu', num_layers_dense=5, num_neurons=100,\n",
    "                                                                  optimizer='Adagrad', lr=0.01, loss='mse', metrics=['mae'],\n",
    "                                                                  patience=0, epochs=500, verbose=0,\n",
    "                                                                  X_test=X_test, y_test=y_test, index=np.arange(0, 464),\n",
    "                                                                  plot_history=True\n",
    "                                                                  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5bb1fde",
   "metadata": {},
   "source": [
    "### <font color='teal'> Conclusiones </font>\n",
    "- Modelos encontrados:\n",
    "    - El mejor modelo encontrado es el de CNN con 0.89 de r2.\n",
    "    - El segundo mejor modelo encontrado es del MLP con 0.88 de r2.\n",
    "- Selección del Modelo\n",
    "    - Se selecciona el modelo de MLP, ya que aunque tiene un punto menos de r2, el tiempo de entrenamiento es mucho menor que el de CNN (aprox. 5.5 veces más)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30f25c92",
   "metadata": {},
   "source": [
    "___\n",
    "## <font color='maroon'> Clasificación de Series </font>\n",
    "### <font color='maroon'> Preparación Dataset </font>\n",
    "- En este paso se realizará el procesamiento para lograr la clasificación de las series de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23da147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las clases existentes\n",
    "data['Clase'].value_counts().plot(kind='bar', figsize=(12, 8), color='teal')\n",
    "plt.title('Clases', fontsize=15)\n",
    "plt.ylabel('Cantidad de productos', fontsize=12)\n",
    "plt.xlabel('Clase', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Conteo\n",
    "print(f'Hay {data[\"Clase\"].nunique()} clases de productos distintas.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86b94851",
   "metadata": {},
   "source": [
    "Debido a que son demasiadas clases y están imbalanceadas, se agruparán los productos en $3$ distintas clasificaciones:\n",
    "+ Frutas\n",
    "+ Verduras\n",
    "+ Otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario con las clases\n",
    "dict_clases = {0: 'Others', 1: 'Fruits', 2: 'Veggies'}\n",
    "\n",
    "# Obteniendo las nuevas categorías\n",
    "data = get_categories(data)\n",
    "# Mostrando\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a832a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo las longitudes de las series de tiempo\n",
    "products_ts = {}\n",
    "for prod in data['Producto'].unique():\n",
    "    products_ts[prod] = [np.array(data.loc[data.Producto==prod]['Promedio']), data.loc[data.Producto==prod]['y'][0]]\n",
    "\n",
    "# Ver las diferentes longitudes de las series de tiempo\n",
    "lens = []\n",
    "for values in products_ts.values():\n",
    "    lens.append(len(values[0]))\n",
    "\n",
    "# Mostrando cantidad de longitudes diferentes que tenemos\n",
    "print(f'Hay {len(set(lens))} longitudes diferentes de series de tiempo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo la cantidad de productos por longitud de serie de tiempo de cada clase\n",
    "plot_length_ts(products_ts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4604d81f",
   "metadata": {},
   "source": [
    "- Al tener series con longitudes diferentes se realizará un padding para que todas las series tengan la misma longitud.\n",
    "- Se decidió que la longitud de las series de tiempo sea de 1250 datos, ya que es el punto medio de las 3 distribuciones.\n",
    "    - Para realizarlo se hará un padding repitiendo el patrón de las series que tengan menos de 1250 datos y se truncarán las series que tengan más de 1250 datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo nuestro dataset transformado, con el umbral de 1250\n",
    "new_data, labels = TransformData(products_ts=products_ts, umbral=1250).apply_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrando 5 ejemplos del nuevo dataset\n",
    "print(f'Datos X: \\n{new_data[:5]}')\n",
    "print(f'\\nDatos y: \\n{labels[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b24ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo en train y test\n",
    "X_train, X_test, y_train, y_test = split_data_clasification(data=new_data, labels=labels, test_size=0.2, shuffle=True)\n",
    "\n",
    "# Lista con las etiquetas\n",
    "etiquetas =  [dict_clases.get(0), dict_clases.get(1), dict_clases.get(2)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b521d82",
   "metadata": {},
   "source": [
    "### <font color='maroon'> Clasificación con Machine Learning </font>\n",
    "#### <font color='teal'> XGBoost </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ccc8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y ajustar el modelo XGBoost\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones con todos los datos\n",
    "y_pred = xgb_model.predict(new_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "get_evaluation('XGBoost', labels, y_pred, etiquetas) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0ba3793",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Naive Bayes </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y ajustar el modelo Naive Bayes\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones con todos los datos\n",
    "y_pred = nb_model.predict(new_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "get_evaluation('Naive Bayes', labels, y_pred, etiquetas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17c17401",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Logistic Regression </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce101cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y ajustar el modelo de regresión logística\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones con todos los datos\n",
    "y_pred = lr.predict(new_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "get_evaluation('Regresión Logística', labels, y_pred, etiquetas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4748f722",
   "metadata": {},
   "source": [
    "#### <font color='teal'> K-Means </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el objeto KMeans\n",
    "kmeans = KMeans(n_clusters=4)  # Ponemos 4 clústeres, porque sabemos que hay 4 etiquetas\n",
    "# Ajusta el modelo con tus datos de entrenamiento\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "# Predice los clústeres de tus datos de prueba\n",
    "y_pred = kmeans.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "get_evaluation_kmeans('KMeans', X_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06c619d0",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Conclusiones </font>\n",
    "- Al tener tan pocos datos el entrenamiento de los modelos es muy rápido, por lo que esta no será una variable a considerar para la selección del modelo.\n",
    "- El mejor modelo encontrado es el de XGBoost con 0.91 de accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a520036",
   "metadata": {},
   "source": [
    "\n",
    "### <font color='maroon'> Clasificación con Deep Learning </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb15283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959b03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96a9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
